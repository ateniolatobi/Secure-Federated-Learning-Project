{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SecureFederatedLearning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhpWfO94kckh",
        "colab_type": "text"
      },
      "source": [
        "#LESSON 8: SECURE FEDERATED LEARNING FOR DEEPLEARNING PROJECT \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfWcYZODk2f_",
        "colab_type": "text"
      },
      "source": [
        "**Author** \n",
        ": Ateniola Oluwatobi Victor\n",
        "\n",
        "**Objective** : My implementation of the Final project in the Secure Federated Learning\n",
        "section of the Secure and Private AI Scholarship Challenge Nanodegree Program using the MNIST digit dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMMwQuKm2k6O",
        "colab_type": "code",
        "outputId": "30bd5418-99aa-4ae9-986a-fb271c3a236f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install syft"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting syft\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/49/1262a301e9db38f4822fa9e0e3b15831c86af9eaeb43b2b0aea3d0c6e1a1/syft-0.1.22a1-py3-none-any.whl (249kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 15.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256kB 2.8MB/s \n",
            "\u001b[?25hCollecting flask-socketio>=3.3.2 (from syft)\n",
            "  Downloading https://files.pythonhosted.org/packages/66/44/edc4715af85671b943c18ac8345d0207972284a0cd630126ff5251faa08b/Flask_SocketIO-4.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tblib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Collecting websocket-client>=0.56.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 44.4MB/s \n",
            "\u001b[?25hCollecting lz4>=2.1.6 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/c6/96bbb3525a63ebc53ea700cc7d37ab9045542d33b4d262d0f0408ad9bbf2/lz4-2.1.10-cp36-cp36m-manylinux1_x86_64.whl (385kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.0)\n",
            "Collecting zstd>=1.4.0.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/37/6a7ba746ebddbd6cd06de84367515d6bc239acd94fb3e0b1c85788176ca2/zstd-1.4.1.0.tar.gz (454kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 45.5MB/s \n",
            "\u001b[?25hCollecting msgpack>=0.6.1 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/7e/ae9e91c1bb8d846efafd1f353476e3fd7309778b582d2fb4cea4cc15b9a2/msgpack-0.6.1-cp36-cp36m-manylinux1_x86_64.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.16.4)\n",
            "Collecting websockets>=7.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/4b/ad228451b1c071c5c52616b7d4298ebcfcac5ae8515ede959db19e4cd56d/websockets-8.0.2-cp36-cp36m-manylinux1_x86_64.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 24.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.21.3)\n",
            "Collecting tf-encrypted!=0.5.7,>=0.5.4 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/ce/da9916e7e78f736894b15538b702c0b213fd5d60a7fd6e481d74033a90c0/tf_encrypted-0.5.6-py3-none-manylinux1_x86_64.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: Flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.1)\n",
            "Collecting python-socketio>=4.3.0 (from flask-socketio>=3.3.2->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/b0/22c3f785f23fec5c7a815f47c55d7e7946a67ae2129ff604148e939d3bdb/python_socketio-4.3.1-py2.py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 19.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.56.0->syft) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->syft) (4.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (0.13.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (1.3.0)\n",
            "Requirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Collecting pyyaml>=5.1 (from tf-encrypted!=0.5.7,>=0.5.4->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 52.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (0.15.5)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (7.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (2.10.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (1.1.0)\n",
            "Collecting python-engineio>=3.9.0 (from python-socketio>=4.3.0->flask-socketio>=3.3.2->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/20/8e3ba16102ae2e245d70d9cb9fa48b076253fdb036dc43eea142294c2897/python_engineio-3.9.3-py2.py3-none-any.whl (119kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 41.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision>=0.3.0->syft) (0.46)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.33.4)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (3.7.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.1.7)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=1.0.2->syft) (1.1.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (2.8.0)\n",
            "Building wheels for collected packages: zstd, pyyaml\n",
            "  Building wheel for zstd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zstd: filename=zstd-1.4.1.0-cp36-cp36m-linux_x86_64.whl size=1067070 sha256=52b34b3210c9d4fb21ed25590756299eb33b39a71395651cedce944fd12088bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/3f/ee/ac08c81af7c1b24a80c746df669ea3cb37542d27877d66ccf4\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp36-cp36m-linux_x86_64.whl size=44105 sha256=2867023dda50fea2fcd0562b3cef8b295235920739a659cbf8eccade7e15ac7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\n",
            "Successfully built zstd pyyaml\n",
            "Installing collected packages: python-engineio, python-socketio, flask-socketio, websocket-client, lz4, zstd, msgpack, websockets, pyyaml, tf-encrypted, syft\n",
            "  Found existing installation: msgpack 0.5.6\n",
            "    Uninstalling msgpack-0.5.6:\n",
            "      Successfully uninstalled msgpack-0.5.6\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed flask-socketio-4.2.1 lz4-2.1.10 msgpack-0.6.1 python-engineio-3.9.3 python-socketio-4.3.1 pyyaml-5.1.2 syft-0.1.22a1 tf-encrypted-0.5.6 websocket-client-0.56.0 websockets-8.0.2 zstd-1.4.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw04gE3_25FV",
        "colab_type": "code",
        "outputId": "7d137b57-0b8e-4057-c0d7-226064830c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "import torch as th\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import Subset\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import syft as sy\n",
        "import helper\n",
        "\n",
        "#Hooking syft to torch\n",
        "hook = sy.TorchHook(th)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0811 16:45:25.221754 139801331033984 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0811 16:45:25.240909 139801331033984 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEjlKiYm3U7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method to create 10 virtual workers and move to a list of workers\n",
        "def create_workers():\n",
        "  workers = []\n",
        "  cartman = sy.VirtualWorker(hook, id = \"cartman\")\n",
        "  workers.append(cartman)\n",
        "  kyle = sy.VirtualWorker(hook, id = \"kyle\")\n",
        "  workers.append(kyle)\n",
        "  kenny = sy.VirtualWorker(hook, id = \"kenny\")\n",
        "  workers.append(kenny)\n",
        "  stan = sy.VirtualWorker(hook, id = \"stan\")\n",
        "  workers.append(stan)\n",
        "  butters = sy.VirtualWorker(hook, id = \"butters\")\n",
        "  workers.append(butters)\n",
        "  wendy = sy.VirtualWorker(hook, id = \"wendy\")\n",
        "  workers.append(wendy)\n",
        "  heidi = sy.VirtualWorker(hook, id = \"heidi\")\n",
        "  workers.append(heidi)\n",
        "  bebe = sy.VirtualWorker(hook, id = \"bebe\")\n",
        "  workers.append(bebe)\n",
        "  nichole = sy.VirtualWorker(hook, id = \"nichole\")\n",
        "  workers.append(nichole)\n",
        "  patty = sy.VirtualWorker(hook, id = \"patty\")\n",
        "  workers.append(patty)\n",
        "  return workers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAxM-Jg23dCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method to clear out every tensor stored in the list of virtual workers\n",
        "def clear_workers(workers):\n",
        "  for worker in workers:\n",
        "    worker.clear_objects()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwrYP-eQ3hdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method to split the mnist test dataset into the various workers and also to load the mnist test dataset into a test loader\n",
        "def create_federated_and_test_loaders(workers, trainset, testset):\n",
        "  federated_train_loader = sy.FederatedDataLoader(\n",
        "      trainset.federate(workers), \n",
        "      batch_size=32, shuffle=True)\n",
        "\n",
        "  test_loader = th.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "  return federated_train_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZIBxk0Z6LNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method to bind remote optimizer functions to the Initial states of the remote models \n",
        "def create_models(workers, lr):\n",
        "  remoteModels = list()\n",
        "  remoteOptimizers = list()\n",
        "  for worker in workers:\n",
        "    model = classifier()\n",
        "    model = model.send(worker)\n",
        "    remoteOptimizers.append(optim.SGD(model.parameters(), lr))\n",
        "    remoteModels.append(model)\n",
        "  return remoteModels, remoteOptimizers\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsT1-pez3l7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method to train models on the virtual workers without moving any gradients to the central model until the gradients have been collated.\n",
        "def create_train_federated_models(workers, loader, lr = 0.12, epoch = 5):\n",
        "  #sends model to first virtual worker\n",
        "  virtual_models, virtual_optimizers = create_models(workers, lr)\n",
        "#   virtual_model = classifier().send(workers[0])\n",
        "#   optimizer = optim.SGD(virtual_model.parameters(), lr)\n",
        "  criterion = nn.NLLLoss()\n",
        "  for n in range(epoch):\n",
        "    \n",
        "    #Integer to keep up with first index.\n",
        "    i = 0\n",
        "    \n",
        "    #Integer to keep up with current worker while training\n",
        "    j = 0\n",
        "    \n",
        "    #Integer to count number of mini-batches per worker\n",
        "    n_mbatch = 0\n",
        "    \n",
        "    #Variable to keep up with current worker while looping\n",
        "    dbLoc = None\n",
        "    \n",
        "    #Variable to store cummulative loss.\n",
        "    cum_loss = 0\n",
        "    \n",
        "    \n",
        "    for batch_idx, (imgs, labels) in enumerate(loader):\n",
        "      \n",
        "      #condition to set dbLoc to the first worker\n",
        "      if i == 0:\n",
        "        i = 2\n",
        "        dbLoc = imgs.location\n",
        "        \n",
        "      #condition to change dbLoc if img is stored on a different worker and also calculate loss\n",
        "      if dbLoc is not imgs.location:\n",
        "        print(\"The total loss for {0} for epoch {2} is {1}\".format(workers[j].id, cum_loss / n_mbatch, n+1))\n",
        "        dbLoc = imgs.location\n",
        "        j += 1\n",
        "        \n",
        "        #Moving the model to a new worker\n",
        "#         virtual_model.move(dbLoc)\n",
        "        \n",
        "        #Resetting the cummulative loss and batch count to zero for new worker\n",
        "        cum_loss = 0\n",
        "        n_mbatch = 0\n",
        "\n",
        "      virtual_optimizers[j].zero_grad()\n",
        "      output = virtual_models[j].forward(imgs)\n",
        "      loss = criterion(output, labels)\n",
        "      loss.backward()\n",
        "      virtual_optimizers[j].step()\n",
        "      cum_loss +=  loss.get().item()\n",
        "      n_mbatch += 1\n",
        "    print(\"The total loss for {0} is {1}\".format(workers[j].id, cum_loss / n_mbatch))\n",
        "  return virtual_models\n",
        "  \n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfJuPL1K3qT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method to return the model to the central machine\n",
        "def create_central_model(model):\n",
        "  return model.get()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KEz8RAnUW8PT",
        "colab": {}
      },
      "source": [
        "#Method to convert the gradients of the remote models to their integar representation, encrypt and distribute among the provided workers\n",
        "def share_gradients(models, workers):\n",
        "  for model in models:\n",
        "    model.fc1.weight.data = model.fc1.weight.data.fix_prec().share(*workers)\n",
        "    model.fc1.bias.data = model.fc1.bias.data.fix_prec().share(*workers)\n",
        "    model.fc2.weight.data = model.fc2.weight.data.fix_prec().share(*workers)\n",
        "    model.fc2.bias.data = model.fc2.bias.data.fix_prec().share(*workers)\n",
        "    model.fc3.weight.data = model.fc3.weight.data.fix_prec().share(*workers)\n",
        "    model.fc3.bias.data = model.fc3.bias.data.fix_prec().share(*workers)\n",
        "    model.fc4.weight.data = model.fc4.weight.data.fix_prec().share(*workers)\n",
        "    model.fc4.bias.data = model.fc4.bias.data.fix_prec().share(*workers)\n",
        "    model.fc5.weight.data = model.fc5.weight.data.fix_prec().share(*workers)\n",
        "    model.fc5.bias.data = model.fc5.bias.data.fix_prec().share(*workers)   \n",
        "  return models\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxj1AZIQX7oO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method to aggregate the gradients accross the shared devices and return the parameters\n",
        "def aggregate_grads(models):\n",
        "  fc1_weight = list()\n",
        "  fc1_bias = list()\n",
        "  fc2_weight = list()\n",
        "  fc2_bias = list()\n",
        "  fc3_weight = list()\n",
        "  fc3_bias = list()\n",
        "  fc4_weight = list()\n",
        "  fc4_bias = list()\n",
        "  fc5_weight = list()\n",
        "  fc5_bias = list()\n",
        "  for model in models:\n",
        "    fc1_weight.append(model.fc1.weight.data.clone().get())\n",
        "    fc1_bias.append(model.fc1.bias.data.clone().get())\n",
        "    fc2_weight.append(model.fc2.weight.data.clone().get())\n",
        "    fc2_bias.append(model.fc2.bias.data.clone().get())\n",
        "    fc3_weight.append(model.fc3.weight.data.clone().get())\n",
        "    fc3_bias.append(model.fc3.bias.data.clone().get())\n",
        "    fc4_weight.append(model.fc4.weight.data.clone().get())\n",
        "    fc4_bias.append(model.fc4.bias.data.clone().get())\n",
        "    fc5_weight.append(model.fc5.weight.data.clone().get())\n",
        "    fc5_bias.append(model.fc5.bias.data.clone().get())\n",
        "  params = {}\n",
        "  params[\"fc1.weight\"] = (sum(fc1_weight) / len(fc1_weight)).get().float_prec()\n",
        "  params[\"fc1.bias\"] = (sum(fc1_bias) / len(fc1_bias)).get().float_prec()\n",
        "  params[\"fc2.weight\"] = (sum(fc2_weight) / len(fc2_weight)).get().float_prec()\n",
        "  params[\"fc2.bias\"] = (sum(fc2_bias) / len(fc2_bias)).get().float_prec()\n",
        "  params[\"fc3.weight\"] = (sum(fc3_weight) / len(fc3_weight)).get().float_prec()\n",
        "  params[\"fc3.bias\"] = (sum(fc3_bias) / len(fc3_bias)).get().float_prec()\n",
        "  params[\"fc4.weight\"] = (sum(fc4_weight) / len(fc4_weight)).get().float_prec()\n",
        "  params[\"fc4.bias\"] = (sum(fc4_bias) / len(fc4_bias)).get().float_prec()\n",
        "  params[\"fc5.weight\"] = (sum(fc5_weight) / len(fc5_weight)).get().float_prec()\n",
        "  params[\"fc5.bias\"] = (sum(fc5_bias) / len(fc5_bias)).get().float_prec()\n",
        "  return params\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxTIsNBSc8Iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Method to update the central model with the aggregated parameters from the remote models\n",
        "def update_central_model(params):\n",
        "  model = classifier()\n",
        "  model.fc1.weight.data = params[\"fc1.weight\"]\n",
        "  model.fc1.bias.data = params[\"fc1.bias\"]\n",
        "  model.fc2.weight.data = params[\"fc2.weight\"]\n",
        "  model.fc2.bias.data = params[\"fc2.bias\"]\n",
        "  model.fc3.weight.data = params[\"fc3.weight\"]\n",
        "  model.fc3.bias.data = params[\"fc3.bias\"]\n",
        "  model.fc4.weight.data = params[\"fc4.weight\"]\n",
        "  model.fc4.bias.data = params[\"fc4.bias\"]\n",
        "  model.fc5.weight.data = params[\"fc5.weight\"]\n",
        "  model.fc5.bias.data = params[\"fc5.bias\"]  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTuBHyru3tU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK-wDJ213xrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Classifier for creating the models\n",
        "class classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__() \n",
        "    self.fc1 = nn.Linear(784, 256)\n",
        "    self.fc2 = nn.Linear(256, 128)\n",
        "    self.fc3 = nn.Linear(128, 64)\n",
        "    self.fc4 = nn.Linear(64, 32)\n",
        "    self.fc5 = nn.Linear(32, 10)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = F.relu(self.fc4(x))\n",
        "    x = F.log_softmax(self.fc5(x), dim = 1)   \n",
        "    return x\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trKb9ytS3F3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Application of transforms to normalize the mnist data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHuUjlvk4CKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "workers = create_workers()\n",
        "clear_workers(workers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMa4Aeny4QD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "federatedset, testset= create_federated_and_test_loaders(workers, mnist_trainset, mnist_testset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqIFB6bX5ANd",
        "colab_type": "code",
        "outputId": "5fb88061-867d-42ba-a5e4-a7c172c29dc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "remoteModels = create_train_federated_models(workers, federatedset, lr = 0.12, epoch = 6)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The total loss for cartman for epoch 1 is 1.888713796405082\n",
            "The total loss for kyle for epoch 1 is 1.9061656721094822\n",
            "The total loss for kenny for epoch 1 is 1.8456643531297117\n",
            "The total loss for stan for epoch 1 is 1.9410718092893033\n",
            "The total loss for butters for epoch 1 is 1.9659484123930018\n",
            "The total loss for wendy for epoch 1 is 1.890808604499127\n",
            "The total loss for heidi for epoch 1 is 1.769944117741382\n",
            "The total loss for bebe for epoch 1 is 1.790674144283254\n",
            "The total loss for nichole for epoch 1 is 1.8149789790523814\n",
            "The total loss for patty is 1.871158241591555\n",
            "The total loss for cartman for epoch 2 is 0.8731461511330402\n",
            "The total loss for kyle for epoch 2 is 0.8791234556664812\n",
            "The total loss for kenny for epoch 2 is 0.875950120785769\n",
            "The total loss for stan for epoch 2 is 0.8649684112281242\n",
            "The total loss for butters for epoch 2 is 0.9783720545312191\n",
            "The total loss for wendy for epoch 2 is 0.9612294272222417\n",
            "The total loss for heidi for epoch 2 is 0.9468026573353625\n",
            "The total loss for bebe for epoch 2 is 0.8371452838182449\n",
            "The total loss for nichole for epoch 2 is 0.9419494161897517\n",
            "The total loss for patty is 0.8193369126541817\n",
            "The total loss for cartman for epoch 3 is 0.5401839661867694\n",
            "The total loss for kyle for epoch 3 is 0.5262921635616333\n",
            "The total loss for kenny for epoch 3 is 0.5630713141662009\n",
            "The total loss for stan for epoch 3 is 0.6896220719877709\n",
            "The total loss for butters for epoch 3 is 0.5655549863710049\n",
            "The total loss for wendy for epoch 3 is 0.5587073249861281\n",
            "The total loss for heidi for epoch 3 is 0.5512754381337064\n",
            "The total loss for bebe for epoch 3 is 0.5460143532366195\n",
            "The total loss for nichole for epoch 3 is 0.5925053684635365\n",
            "The total loss for patty is 0.47245897495366157\n",
            "The total loss for cartman for epoch 4 is 0.38558214466939583\n",
            "The total loss for kyle for epoch 4 is 0.4431596005216558\n",
            "The total loss for kenny for epoch 4 is 0.43109107635756755\n",
            "The total loss for stan for epoch 4 is 0.3947578812453975\n",
            "The total loss for butters for epoch 4 is 0.4336123430031411\n",
            "The total loss for wendy for epoch 4 is 0.4063870110251802\n",
            "The total loss for heidi for epoch 4 is 0.39675933641797684\n",
            "The total loss for bebe for epoch 4 is 0.402666297087327\n",
            "The total loss for nichole for epoch 4 is 0.43725555322747284\n",
            "The total loss for patty is 0.32743852316065036\n",
            "The total loss for cartman for epoch 5 is 0.3257438238076073\n",
            "The total loss for kyle for epoch 5 is 0.3079183856223492\n",
            "The total loss for kenny for epoch 5 is 0.31857783450408184\n",
            "The total loss for stan for epoch 5 is 0.3022600358867265\n",
            "The total loss for butters for epoch 5 is 0.3621783371856238\n",
            "The total loss for wendy for epoch 5 is 0.3267481234717242\n",
            "The total loss for heidi for epoch 5 is 0.32938397802571034\n",
            "The total loss for bebe for epoch 5 is 0.3286079257885192\n",
            "The total loss for nichole for epoch 5 is 0.3748188970333084\n",
            "The total loss for patty is 0.24829039449228885\n",
            "The total loss for cartman for epoch 6 is 0.23140087117381553\n",
            "The total loss for kyle for epoch 6 is 0.2485011435569601\n",
            "The total loss for kenny for epoch 6 is 0.30810310013909287\n",
            "The total loss for stan for epoch 6 is 0.2745572840596767\n",
            "The total loss for butters for epoch 6 is 0.2840394693961803\n",
            "The total loss for wendy for epoch 6 is 0.3018802253806845\n",
            "The total loss for heidi for epoch 6 is 0.25359328114923013\n",
            "The total loss for bebe for epoch 6 is 0.2558646572634895\n",
            "The total loss for nichole for epoch 6 is 0.28457002888651606\n",
            "The total loss for patty is 0.21062321810329215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkqjhLfpBSx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "remoteModels = share_gradients(remoteModels, workers[5:8])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a87vge3DR59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params  = aggregate_grads(remoteModels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGiYHGc5rrpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "central_model = update_central_model(params)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hooScohZJ6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}